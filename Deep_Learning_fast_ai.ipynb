{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyN/OhjhoqC1xm21MQIqn1YB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AwaisAli37405/Deep_Learning_with_fast_ai/blob/master/Deep_Learning_fast_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsNzuRfjxIWd"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import *\n",
        "from fastai.text.all import *\n",
        "from fastai.collab import *\n",
        "from fastai.tabular.all import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U fastprogress fastai"
      ],
      "metadata": {
        "id": "6a05x5uvIw-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In order to use the built in datasets available in fast ai untar_data. It can be downloaded and decompresses using the following line of code:"
      ],
      "metadata": {
        "id": "wrfkmOXc7Qzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.PETS)/'images'\n",
        "# print(path)\n",
        "# path.ls()"
      ],
      "metadata": {
        "id": "iEPIKqUkQSMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IT will download the dataset once and will return the location. Now we will use the factory method that is a great way to get your data quickly ready for training - get image file. It is a fastai function that helps us grab all the image files (recursively) in one folder."
      ],
      "metadata": {
        "id": "cX4ZNqxs7pu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = get_image_files(path)\n",
        "len(files)\n"
      ],
      "metadata": {
        "id": "H42nm_DNPLbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now to get this data labeled following convention has been adopted in fast.ai. There is an easy way to distinguish: the name of the file begins with a capital for cats, and a lowercased letter for dogs"
      ],
      "metadata": {
        "id": "vtl117818pYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_cat(x): return x[0].isupper()\n"
      ],
      "metadata": {
        "id": "5RHM6SjP8qDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To get our data ready for the model we need to initialize the dataloader object.\n",
        "### There is function in Vision_data of fast_ai that can label the examples using names of the image files imagedataloader.from_name_fucntion"
      ],
      "metadata": {
        "id": "0oe7hx4A9iWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We have passed to this function the directory we’re working in, the files we grabbed, our label_func and one last piece as item_tfms: this is a Transform applied on all items of our dataset that will resize each image to 224 by 224, by using a random crop on the largest dimension to make it a square, then resizing to 224 by 224. If we didn’t pass this, we would get an error later as it would be impossible to batch the items together."
      ],
      "metadata": {
        "id": "YN6zpbL6-HRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dls = ImageDataLoaders.from_name_func(\n",
        "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
        "    label_func=is_cat, item_tfms=Resize(224))\n",
        "\n"
      ],
      "metadata": {
        "id": "GRbR_fYdH8fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dls.show_batch()"
      ],
      "metadata": {
        "id": "gR6AURp6-cm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Then we can create a Learner, which is a fastai object that combines the data and a model for training, and uses transfer learning to fine tune a pretrained model in just two lines of code:"
      ],
      "metadata": {
        "id": "eP2OQDQH-yyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
        "learn.fine_tune(1)"
      ],
      "metadata": {
        "id": "BJnCwUQCIF1_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "3e1c3bbf-62dc-43e9-ecf0-f203171b31a1"
      },
      "execution_count": 73,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 104MB/s] \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><div><progress max=\"1\" value=\"0\"></progress> 0.00% [0/1 00:00&lt;?]</div><div><table class=\"fastprogress\"><thead><tr><th>epoch</th><th>train_loss</th><th>valid_loss</th><th>error_rate</th><th>time</th></tr></thead><tbody></tbody></table></div><div><progress max=\"92\" value=\"48\"></progress> 52.17% [48/92 15:26&lt;14:09... 0.3614]</div></div>"
            ],
            "text/markdown": "```html\n<div>\n  <div>\n<progress max=\"1\" value=\"0\"></progress> 0.00% [0/1 00:00&lt;?]  </div>\n  <div>\n    <table class=\"fastprogress\">\n      <thead>\n        <tr>\n          <th>epoch</th>\n          <th>train_loss</th>\n          <th>valid_loss</th>\n          <th>error_rate</th>\n          <th>time</th>\n        </tr>\n      </thead>\n      <tbody></tbody>\n    </table>\n  </div>\n  <div>\n<progress max=\"92\" value=\"48\"></progress> 52.17% [48/92 15:26&lt;14:09... 0.3614]  </div>\n</div>\n\n```",
            "text/plain": [
              "<div><div><progress max=\"1\" value=\"0\"></progress> 0.00% [0/1 00:00&lt;?]</div><div><table class=\"fastprogress\"><thead><tr><th>epoch</th><th>train_loss</th><th>valid_loss</th><th>error_rate</th><th>time</th></tr></thead><tbody></tbody></table></div><div><progress max=\"92\" value=\"48\"></progress> 52.17% [48/92 15:26&lt;14:09... 0.3614]</div></div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-825105479.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvision_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet34\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfine_tune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastai/callback/schedule.py\u001b[0m in \u001b[0;36mfine_tune\u001b[0;34m(self, epochs, base_lr, freeze_epochs, lr_mult, pct_start, div, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;34m\"Fine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreeze_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpct_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0mbase_lr\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastai/callback/schedule.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    119\u001b[0m     scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),\n\u001b[1;32m    120\u001b[0m               'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mParamScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_opt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m# %% ../../nbs/14_callback.schedule.ipynb #a7f5140e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_hypers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelFitException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end_cleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_end_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_opt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelTrainException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_one_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelBatchException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_grad_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_grad_opt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_grad_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'backward'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelBackwardException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelStepException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    619\u001b[0m         \"\"\"\n\u001b[1;32m    620\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m             return handle_torch_function(\n\u001b[0m\u001b[1;32m    622\u001b[0m                 \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/overrides.py\u001b[0m in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0;31m# Use `public_api` instead of `implementation` so __torch_function__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m         \u001b[0;31m# implementations can do equality/identity comparisons.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1763\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_func_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpublic_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastai/torch_core.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'__str__'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_torch_handled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mdict_objs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_find_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTensorBase\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdict_objs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_objs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mas_copy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1703\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1704\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_default_nowrap_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             )\n\u001b[0;32m--> 630\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    866\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If you want to make a prediction on a new image, you can use learn.predict:"
      ],
      "metadata": {
        "id": "DXrJLVxq-4jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn.predict(files[0])"
      ],
      "metadata": {
        "id": "l35xAmJK-0Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.show_results()"
      ],
      "metadata": {
        "id": "uB3IVEmF--rD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to export the models\n",
        "learn.export('model.pkl')"
      ],
      "metadata": {
        "id": "oxFXmkA0lbFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "?vision_learner"
      ],
      "metadata": {
        "id": "xY4Fx_CgBnBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Which Image models are the best?\n",
        "Pytorch has many [image models](https://timm.fast.ai/) around 500. These models are basically the mathemical functions differ on how much time, accuracy and from which family they belong.\n",
        "\n",
        "Ross regularly [benchmarks](https://www.kaggle.com/code/jhoward/which-image-models-are-best/) new models as they are added to timm."
      ],
      "metadata": {
        "id": "aAHeg_aNgU7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timm"
      ],
      "metadata": {
        "id": "Fm2CsMYugj6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timm.list_models() # list all the models in pytorch"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PvDkBQA8kUqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we want to use the covnext model - Search for model architectures by Wildcard\n",
        "timm.list_models('convnext*')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "57LU_nE_kXN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "??(vision_learner)\n"
      ],
      "metadata": {
        "id": "pEbS7VWOmfoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normally, models perform calculations using fp32 (Single-Precision), where each number takes up 32 bits of memory. By calling .to_fp16() on your vision_learner, you are telling the model to use Mixed Precision training.\n",
        "$$\n",
        "\\begin{array}{|l|c|c|}\n",
        "\\hline\n",
        "\\textbf{Feature} & \\textbf{FP32 (Standard)} & \\textbf{FP16 (Mixed Precision)} \\\\ \\hline\n",
        "\\text{Memory per value} & \\text{4 Bytes (32 bits)} & \\text{2 Bytes (16 bits)} \\\\ \\hline\n",
        "\\text{Training Speed} & \\text{Baseline} & \\text{Significantly Faster} \\\\ \\hline\n",
        "\\text{VRAM Usage} & \\text{Higher} & \\text{Lower (~50\\% less)} \\\\ \\hline\n",
        "\\text{Hardware Requirement} & \\text{Any GPU} & \\text{Modern GPU (Turing+)} \\\\ \\hline\n",
        "\\end{array}\n",
        "$$"
      ],
      "metadata": {
        "id": "sJ3vR2WdooZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# so if we want to use any of these models with fast-ai vision learner we have to provide it as a string as an input\n",
        "learn = vision_learner(dls, 'convnext_tiny_in22k', metrics=error_rate).to_fp16()\n",
        "learn.fine_tune(1)"
      ],
      "metadata": {
        "id": "pyChC7PjkmdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the `dls.vocab` Object\n",
        "\n",
        "In fastai, the `dls.vocab` (Vocabulary) acts as the mapping system between human-readable labels and the integer indices used by the neural network.\n",
        "\n",
        "$$\n",
        "\\begin{array}{|l|l|l|}\n",
        "\\hline\n",
        "\\textbf{Property/Method} & \\textbf{Description} & \\textbf{Example Output} \\\\ \\hline\n",
        "\\text{dls.vocab} & \\text{The list of unique class names} & \\text{['black', 'grizzly', 'teddy']} \\\\ \\hline\n",
        "\\text{len(dls.vocab)} & \\text{Total number of classes (output neurons)} & \\text{3} \\\\ \\hline\n",
        "\\text{dls.vocab[i]} & \\text{Find label string at index } i & \\text{'grizzly' (for } i=1\\text{)} \\\\ \\hline\n",
        "\\text{dls.vocab.o2i} & \\text{Dictionary mapping 'Object to Index'} & \\text{\\{'black': 0, 'grizzly': 1, ...\\}} \\\\ \\hline\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "**Key Mathematical Concept:**\n",
        "The final layer of your model outputs a vector $\\mathbf{y}$ of size $N$, where $N = \\text{len(dls.vocab)}$.\n",
        "The probability for class $i$ is calculated such that:\n",
        "$$ P(\\text{class}_i) = \\text{softmax}(\\mathbf{y})_i $$\n",
        "where the index $i$ corresponds exactly to the position in `dls.vocab`."
      ],
      "metadata": {
        "id": "NkjQCnIcpoG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories  =  learn.dls.vocab"
      ],
      "metadata": {
        "id": "JF95SmcPlRcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(categories) # since we have tewo categories false and true"
      ],
      "metadata": {
        "id": "1xRUCzNMp3IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now to map them\n",
        "def classify_img(img):\n",
        "    pred, idx, probs = learn.predict(img)\n",
        "    return dict(zip(categories, map(float, probs)))"
      ],
      "metadata": {
        "id": "tm3YrlXQp4_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify_img(files[100])"
      ],
      "metadata": {
        "id": "oFaVE7LGqMS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now if you want to look at the trianed model\n",
        "model = learn.model"
      ],
      "metadata": {
        "id": "bpEva9T1rT1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7r3ywa4Wr0b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = model.get_submodule('0.model.stem.1')"
      ],
      "metadata": {
        "id": "uP6pDTipsJDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l"
      ],
      "metadata": {
        "id": "WI15X2TpsXCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(l.parameters())"
      ],
      "metadata": {
        "id": "iILo0M-5sXcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How does really a neural networks work\n",
        "A neural network is just a mathematical function. In the most standard kind of neural network, the function:\n",
        "\n",
        "1.   Multiplies each input by a number of values. These values are known as parameters\n",
        "2.   Adds them up for each group of values\n",
        "3.  Replaces the negative numbers with zeros\n",
        "\n",
        "This represents one \"layer\". Then these three steps are repeated, using the outputs of the previous layer as the inputs to the next layer. Initially, the parameters in this function are selected randomly. Therefore a newly created neural network doesn't do anything useful at all -- it's just random!\n",
        "\n",
        "To get the function to \"learn\" to do something useful, we have to change the parameters to make them \"better\" in some way. We do this using gradient descent. Let's see how this works..."
      ],
      "metadata": {
        "id": "chY-TOiYJRDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import title\n",
        "# from ipywidgets import interact\n",
        "from fastai.basics import *\n",
        "\n",
        "# plotting a quadratic line\n",
        "def plotting(f, color='r', min=-2,max=2, Title=None):\n",
        "  x = torch.linspace(min, max, 500)[:,None]\n",
        "  plt.plot(x, f(x), color)\n",
        "  plt.title(Title)\n",
        "  plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "-Lsw6-eWsb1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  return 3*x**2 + 2*x +1"
      ],
      "metadata": {
        "id": "Tm_y5n7duPEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotting(f,min=-2,max=2)"
      ],
      "metadata": {
        "id": "Kjo6df9nMulp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This quadratic is of the form $ax^2 + bx + c$, with parameters $a=3$, $b=2$, $c=1$.\n",
        "\n",
        "To make it easier to try out different quadratics for fitting a model to the data we'll create, let's create a function that calculates the value of a point on any quadratic:"
      ],
      "metadata": {
        "id": "84B3A-azOyHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quad(a, b, c, x): return a*x**2 + b*x + c\n"
      ],
      "metadata": {
        "id": "FTSxfJKbNAYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## If we fix some particular values of a, b, and c, then we'll have made a quadratic. To fix values passed to a function in python, we use the partial function, like so:\n",
        "\n"
      ],
      "metadata": {
        "id": "Y7UA5_8ePA0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to put this explanation in your notebook:$$\\underbrace{f(a, b, c, x)}_{\\text{General Function}} \\xrightarrow{\\text{partial}(a,b,c)} \\underbrace{f_{a,b,c}(x)}_{\\text{Specialized Function}}$$For example, if you run f = mk_quad(3, 2, 1), then:f(1) is the same as calling quad(3, 2, 1, 1).f(10) is the same as calling quad(3, 2, 1, 10)."
      ],
      "metadata": {
        "id": "FkM4nYk5dSMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mk_quad(a,b,c): return partial(quad, a,b,c)"
      ],
      "metadata": {
        "id": "mYta40dcO8ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So for instance, we can recreate our previous quadratic:"
      ],
      "metadata": {
        "id": "_v_9jjXhPHKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f2 = mk_quad(3,2,1)\n",
        "plotting(f2)"
      ],
      "metadata": {
        "id": "gBphpr1fPDz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's simulate making some noisy measurements of our quadratic f. We'll then use gradient descent to see if we can recreate the original function from the data.\n",
        "\n",
        "Here's a couple of functions to add some random noise to data:"
      ],
      "metadata": {
        "id": "kRPQ0AM2PodM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\n",
        "def add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)"
      ],
      "metadata": {
        "id": "Eiw82Dh8PLM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "x = torch.linspace(-2, 2, steps=20)[:,None]\n",
        "y = add_noise(f(x), 0.15, 1.5)"
      ],
      "metadata": {
        "id": "9rZ3er0RPxzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A link to numpy book [link](https://wesmckinney.com/book/)"
      ],
      "metadata": {
        "id": "GrDmr_XMQJzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x,y)"
      ],
      "metadata": {
        "id": "_-waPp8UPz6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @interact(a=1.1, b=1.1, c=1.1)\n",
        "def plot_quad(a, b, c):\n",
        "    plt.scatter(x,y)\n",
        "    plotting(mk_quad(a,b,c))"
      ],
      "metadata": {
        "id": "wFzZRjlpQa79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mae(preds, acts): return (torch.abs(preds-acts)).mean()\n"
      ],
      "metadata": {
        "id": "TG62Ic8BQmOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @interact(a=1.1, b=1.1, c=1.1)\n",
        "def plot_quad(a, b, c):\n",
        "    f = mk_quad(a,b,c)\n",
        "    plt.scatter(x,y)\n",
        "    loss = mae(f(x), y)\n",
        "    plotting(f,Title = (f'Loss: {loss:.2f}'))"
      ],
      "metadata": {
        "id": "CPxFhegOQzbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derivatives, which measure the rate of change of a function. Tutorial can be found [here](https://www.youtube.com/playlist?list=PLybg94GvOJ9ELZEe9s2NXTKr41Yedbw7M)"
      ],
      "metadata": {
        "id": "UhwXs5i2Rybd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Define the basic ReLU\n",
        "def relu(x): return torch.clamp(x, min=0.)\n",
        "\n",
        "# 2. Create two different \"neurons\"\n",
        "# Neuron A: Starts at -1.0\n",
        "def neuron_a(x): return relu(x + 1.0)\n",
        "\n",
        "# Neuron B: Starts at 0.5 and is \"flipped\" and steeper\n",
        "def neuron_b(x): return -2.0 * relu(x - 0.5)\n",
        "\n",
        "# 3. The \"Neural Network\" (Adding them together)\n",
        "def neural_net(x): return neuron_a(x) + neuron_b(x)\n",
        "\n",
        "# 4. Plotting the results\n",
        "x = torch.linspace(-2, 2, 100)\n",
        "plt.plot(x, neuron_a(x), '--', label=\"Neuron A\", color='blue')\n",
        "plt.plot(x, neuron_b(x), '--', label=\"Neuron B\", color='green')\n",
        "plt.plot(x, neural_net(x), label=\"Combined Output (The Net)\", color='red', linewidth=3)\n",
        "plt.axhline(0, color='black', lw=1)\n",
        "plt.legend()\n",
        "plt.title(\"How ReLUs Build Shapes\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CrYgGdKAQ2Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be exact, we'll discuss the roles of arrays and tensors and of broadcasting, a powerful technique for using them expressively. We'll explain stochastic gradient descent (SGD), the mechanism for learning by updating weights automatically. We'll discuss the choice of a loss function for our basic classification task, and the role of mini-batches. We'll also describe the math that a basic neural network is actually doing. Finally, we'll put all these pieces together."
      ],
      "metadata": {
        "id": "lQ7JLC1pkYQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How images are represented in a computer"
      ],
      "metadata": {
        "id": "oeaVvKNNklO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "from fastai.vision.all import *\n",
        "import fastbook\n",
        "from fastbook import *\n",
        "\n",
        "matplotlib.rc('image', cmap='Greys')"
      ],
      "metadata": {
        "id": "THE1eOP9neAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.MNIST_SAMPLE)"
      ],
      "metadata": {
        "id": "0e-40hbVfCsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Path.BASE_PATH = path"
      ],
      "metadata": {
        "id": "gJeujKpAnXvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path.ls()"
      ],
      "metadata": {
        "id": "F2O5JONxn7QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path.ls()[0]"
      ],
      "metadata": {
        "id": "Ulhl8PDbo-Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(path/'train').ls()"
      ],
      "metadata": {
        "id": "_nDTxFW0oUqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = get_image_files(path)"
      ],
      "metadata": {
        "id": "a8XyPKZrovad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "threes = (path/'train'/'3').ls().sorted()\n",
        "sevens = (path/'train'/'7').ls().sorted()"
      ],
      "metadata": {
        "id": "gP_geCyjpdvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threes"
      ],
      "metadata": {
        "id": "_KA5zp-MpenW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are using the Image class from the Python Imaging Library (PIL), which is the most widely used Python package for opening, manipulating, and viewing images. Jupyter knows about PIL images, so it displays the image for us automatically."
      ],
      "metadata": {
        "id": "VWJpoi1NqUzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "im3 = threes[1]\n",
        "print(threes[1])\n",
        "Image.open(im3)"
      ],
      "metadata": {
        "id": "vGgYnVOppmwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a computer, everything is represented as a number. To view the numbers that make up this image, we have to convert it to a NumPy array or a PyTorch tensor. For instance, here's what a section of the image looks like, converted to a NumPy array:"
      ],
      "metadata": {
        "id": "TRoLR_IAvnYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array3 = np.array(Image.open(im3))[4:10,4:10]\n",
        "array3"
      ],
      "metadata": {
        "id": "K7eR6i3OvnCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(Image.open(im3)))\n",
        "tensor(Image.open(im3))[4:10,4:10]"
      ],
      "metadata": {
        "id": "r1ZkOlEEp8mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can slice the array to pick just the part with the top of the digit in it, and then use a Pandas DataFrame to color-code the values using a gradient, which shows us clearly how the image is created from the pixel values:"
      ],
      "metadata": {
        "id": "xjQq578Ewj5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "im3_t = tensor(Image.open(im3))\n",
        "df = pd.DataFrame(im3_t)\n",
        "df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')"
      ],
      "metadata": {
        "id": "3ib8Ep-pv9eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the background white pixels are stored as the number 0, black is the number 255, and shades of gray are between the two. The entire image contains 28 pixels across and 28 pixels down, for a total of 784 pixels."
      ],
      "metadata": {
        "id": "v9GsjRC6w9r3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now creating the tensor of images of 3 and 7\n",
        "sevens_tensors = [tensor(Image.open(o)) for o in sevens]\n",
        "threes_tensors = [tensor(Image.open(o)) for o in threes]\n",
        "print(len(sevens_tensors), len(threes_tensors))"
      ],
      "metadata": {
        "id": "aGdXRWrCwo6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also check that one of the images looks okay. Since we now have tensors (which Jupyter by default will print as values), rather than PIL images (which Jupyter by default will display as images), we need to use fastai's show_image function to display it:"
      ],
      "metadata": {
        "id": "P6DXPUD2zaDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_image(sevens_tensors[5])"
      ],
      "metadata": {
        "id": "qRx6Mo8UyO7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sevens_tensors[0].shape"
      ],
      "metadata": {
        "id": "y5YZHvoczMFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now stack the tensors and scaling to 0 and 1 Large numbers (like 255) can cause \"gradient explosions\" during training,\n",
        "#making the model unstable. By dividing the entire stack by 255, you scale every single pixel\n",
        "\n",
        "stacked_sevens = torch.stack(sevens_tensors).float()/255\n",
        "stacked_threes = torch.stack(threes_tensors).float()/255"
      ],
      "metadata": {
        "id": "VQagcIY3zt0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_sevens.shape\n",
        "stacked_sevens.ndim"
      ],
      "metadata": {
        "id": "lz4G92Xw077r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you stacked your images, you created a rank-3 tensor with a shape like $(6131, 28, 28)$.\n",
        "Dimension 0: This is the \"index\" or \"page number\" in your stack (the 6,131 images).\n",
        "Dimension 1: The vertical rows of pixels (28 rows).\n",
        "Dimension 2: The horizontal columns of pixels (28 columns).When you tell PyTorch to take the mean(0), you are telling it: \"Look through the entire depth of the stack at every single $(x, y)$ coordinate and average the values you find there.\""
      ],
      "metadata": {
        "id": "BDiT2Ij99ZLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean3 = stacked_threes.mean(0)\n",
        "show_image(mean3)"
      ],
      "metadata": {
        "id": "v16a-j0Q1L0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean7 = stacked_sevens.mean(0)\n",
        "show_image(mean7)\n"
      ],
      "metadata": {
        "id": "jdYtauwu9zFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How can we determine its distance from our ideal 3? We can't just add up the differences between the pixels of this image and the ideal digit. Some differences will be positive while others will be negative, and these differences will cancel out, resulting in a situation where an image that is too dark in some places and too light in others might be shown as having zero total differences from the ideal. That would be misleading!\n",
        "\n",
        "To avoid this, there are two main ways data scientists measure distance in this context:\n",
        "\n",
        "1. Take the mean of the absolute value of differences (absolute value is the function that replaces negative values with positive values). This is called the mean absolute difference or L1 norm\n",
        "\n",
        "2. Take the mean of the square of differences (which makes everything positive) and then take the square root (which undoes the squaring). This is called the root mean squared error (RMSE) or L2 norm."
      ],
      "metadata": {
        "id": "yIF1LJlbAobS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image = stacked_sevens[0]\n",
        "\n",
        "#calculate its distance\n",
        "l1_norm = (sample_image - mean7).abs().mean()\n",
        "l1_norm"
      ],
      "metadata": {
        "id": "SC0N-GWc92_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l2_norm = ((sample_image - mean7)**2).mean().sqrt()\n",
        "l2_norm"
      ],
      "metadata": {
        "id": "J7JheKGQBHVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch already provides both of these as loss functions. You'll find these inside torch.nn.functional, which the PyTorch team recommends importing as F (and is available by default under that name in fastai):"
      ],
      "metadata": {
        "id": "WwjadLJzCLLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now using the above loss function using tensor flow built in methods\n",
        "F.l1_loss(sample_image, mean7)\n"
      ],
      "metadata": {
        "id": "uCxzDoILBaLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F.mse_loss(sample_image,mean7).sqrt()"
      ],
      "metadata": {
        "id": "hleBbHW-CVjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Playing around with numpy and pytorch tensors"
      ],
      "metadata": {
        "id": "GlUCxml2JMw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [[1,2,3],[4,5,6]]\n",
        "arr = array (data)\n",
        "tns = tensor(data)"
      ],
      "metadata": {
        "id": "eMORCSVWCdA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr"
      ],
      "metadata": {
        "id": "0KUVYUAGJRjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tns"
      ],
      "metadata": {
        "id": "XVDa7httJSp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr.shape"
      ],
      "metadata": {
        "id": "ZpZGhin7JTuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tns.shape"
      ],
      "metadata": {
        "id": "CMqnAUlMJquE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tns[1]"
      ],
      "metadata": {
        "id": "7gCzt2slJrm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tns[1,1:3]"
      ],
      "metadata": {
        "id": "JBchp1ykJwwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tns+1"
      ],
      "metadata": {
        "id": "wa-AiZtAJzZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tns.type()"
      ],
      "metadata": {
        "id": "raNau3YzJ2Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#will automatically change the type\n",
        "tns*1.5"
      ],
      "metadata": {
        "id": "Y3qSoEYZJ3qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tns"
      ],
      "metadata": {
        "id": "L4C6xrZRJ-uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing Metrics using Broadcasting"
      ],
      "metadata": {
        "id": "8kuRbezb1G0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the validationsets\n",
        "valid3_tensor = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls().sorted()])\n",
        "valid7_tensor = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls().sorted()])"
      ],
      "metadata": {
        "id": "KSSC-WqzKALh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid3_tensor = valid3_tensor.float()/255\n",
        "valid7_tensor = valid7_tensor.float()/255"
      ],
      "metadata": {
        "id": "5Zg-qEYm2GkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid3_tensor.shape, valid7_tensor.shape"
      ],
      "metadata": {
        "id": "ZzFWruQl5hay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to subtract each validation image from the mean image calculated above.\n",
        "\n",
        "By using mean((-1, -2)), you are asking: \"For every image in my stack, how far away is it from the ideal?\" This allows you to then compare those distances to see if a specific image looks more like a 3 or a 7.\n",
        "\n",
        "This is what you saw with (-1, -2). Just like a Python list where my_list[-1] is the last item, PyTorch counts backward from the end of the shape:\n",
        "\n",
        "-1: The very last dimension (Columns / 28)\n",
        "\n",
        "-2: The second-to-last dimension (Rows / 28)\n",
        "\n",
        "-3: The third-to-last dimension (Number of images / 6131)"
      ],
      "metadata": {
        "id": "9y6oN7k8YZJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating the mean abs error\n",
        "def distance_l1(a,b):\n",
        "  return (a-b).abs().mean(dim = (-1,-2))"
      ],
      "metadata": {
        "id": "nLSCfPtd5siY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distance_l1(sample_image, mean7)\n"
      ],
      "metadata": {
        "id": "T1ABelLVbCTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But in order to calculate a metric for overall accuracy, we will need to calculate the distance to the ideal 3 for every image in the validation set. How do we do that calculation? We could write a loop over all of the single-image tensors that are stacked within our validation set tensor, valid_3_tens, which has a shape of [1010,28,28] representing 1,010 images. But there is a better way.\n",
        "\n",
        "Something very interesting happens when we take this exact same distance function, designed for comparing two single images, but pass in as an argument valid_3_tens, the tensor that represents the 3s validation set:"
      ],
      "metadata": {
        "id": "7lpWwSltbbgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid3_tensor_dist = distance_l1(valid3_tensor, mean3)\n",
        "valid3_tensor_dist"
      ],
      "metadata": {
        "id": "1iifvypGbSTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of complaining about shapes not matching, it returned the distance for every single image as a vector (i.e., a rank-1 tensor) of length 1,010 (the number of 3s in our validation set). How did that happen?\n",
        "\n",
        "Take another look at our function mnist_distance, and you'll see we have there the subtraction (a-b). The magic trick is that PyTorch, when it tries to perform a simple subtraction operation between two tensors of different ranks, will use broadcasting. That is, it will automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank. Broadcasting is an important capability that makes tensor code much easier to write.\n",
        "\n",
        "After broadcasting so the two argument tensors have the same rank, PyTorch applies its usual logic for two tensors of the same rank: it performs the operation on each corresponding element of the two tensors, and returns the tensor result. For instance:"
      ],
      "metadata": {
        "id": "UrGnIXaqbkhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor([1,2,3]) + tensor(1)"
      ],
      "metadata": {
        "id": "tmnhfUPLbjwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So in this case, PyTorch treats mean3, a rank-2 tensor representing a single image, as if it were 1,010 copies of the same image, and then subtracts each of those copies from each 3 in our validation set. What shape would you expect this tensor to have"
      ],
      "metadata": {
        "id": "sKIltep0bv21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(valid3_tensor - mean3).shape"
      ],
      "metadata": {
        "id": "w9onDV5NbqvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are calculating the difference between our \"ideal 3\" and each of the 1,010 3s in the validation set, for each of 28×28 images, resulting in the shape [1010,28,28].\n",
        "\n",
        "There are a couple of important points about how broadcasting is implemented, which make it valuable not just for expressivity but also for performance:\n",
        "\n",
        "PyTorch doesn't actually copy mean3 1,010 times. It pretends it were a tensor of that shape, but doesn't actually allocate any additional memory\n",
        "It does the whole calculation in C (or, if you're using a GPU, in CUDA, the equivalent of C on the GPU), tens of thousands of times faster than pure Python (up to millions of times faster on a GPU!).\n",
        "This is true of all broadcasting and elementwise operations and functions done in PyTorch. It's the most important technique for you to know to create efficient PyTorch code.\n",
        "\n",
        "Next in mnist_distance we see abs. You might be able to guess now what this does when applied to a tensor. It applies the method to each individual element in the tensor, and returns a tensor of the results (that is, it applies the method \"elementwise\"). So in this case, we'll get back 1,010 matrices of absolute values.\n",
        "\n",
        "Finally, our function calls mean((-1,-2)). The tuple (-1,-2) represents a range of axes. In Python, -1 refers to the last element, and -2 refers to the second-to-last. So in this case, this tells PyTorch that we want to take the mean ranging over the values indexed by the last two axes of the tensor. The last two axes are the horizontal and vertical dimensions of an image. After taking the mean over the last two axes, we are left with just the first tensor axis, which indexes over our images, which is why our final size was (1010). In other words, for every image, we averaged the intensity of all the pixels in that image."
      ],
      "metadata": {
        "id": "gHuoqEQVdfEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distance_l1(valid3_tensor, mean3).shape"
      ],
      "metadata": {
        "id": "W5_0sb3mby9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_3(x): return distance_l1(x, mean3) < distance_l1(x, mean7)"
      ],
      "metadata": {
        "id": "6pakUCHUda28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_3 = is_3(valid3_tensor).float().mean()\n",
        "accuracy_7 = (1 - is_3(valid7_tensor).float()).mean()"
      ],
      "metadata": {
        "id": "8P1eY1kFfX4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_3,accuracy_7,(accuracy_3+accuracy_7)/2"
      ],
      "metadata": {
        "id": "ygsZ-kF6fmH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stocastic Gradient Decent"
      ],
      "metadata": {
        "id": "J1zJ6fRslJ-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the gradients will tell us how much we have to change each weight to make our model better.\n",
        "\n",
        "You may remember from your high school calculus class that the derivative of a function tells you how much a change in its parameters will change its result.\n",
        "\n",
        "One important thing to be aware of is that our function has lots of weights that we need to adjust, so when we calculate the derivative we won't get back one number, but lots of them—a gradient for every weight. But there is nothing mathematically tricky here; you can calculate the derivative with respect to one weight, and treat all the other ones as constant, then repeat that for each other weight. This is how all of the gradients are calculated, for every weight.\n",
        "\n"
      ],
      "metadata": {
        "id": "OUu3W9T-vHkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "byVays5n57gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''If you define function(x) as (x**2).sum(), then the result y = function(xt)\n",
        "will already be a scalar. Because y is now a scalar tensor, you can directly\n",
        "call y.backward() without needing an additional .sum() before it.\n",
        "\n",
        "Let's break down why the gradients remain the same:\n",
        "\n",
        "Original Scenario (function(x) returns x**2):\n",
        "\n",
        "xt = tensor([3.0, 2.0])\n",
        "y_intermediate = xt**2 -> tensor([9.0, 4.0]) (Non-scalar)\n",
        "y_scalar = y_intermediate.sum() -> tensor(13.0) (Scalar)\n",
        "y_scalar.backward() calculates the gradient of (x_1^2 + x_2^2) with respect to x_1 and x_2.\n",
        "d(x_1^2 + x_2^2)/dx_1 = 2*x_1 = 2*3 = 6\n",
        "d(x_1^2 + x_2^2)/dx_2 = 2*x_2 = 2*2 = 4\n",
        "xt.grad will be tensor([6.0, 4.0]).\n",
        "Your Proposed Scenario (function(x) returns (x**2).sum()):\n",
        "\n",
        "xt = tensor([3.0, 2.0])\n",
        "y = function(xt) -> (xt**2).sum() -> (tensor([9.0, 4.0])).sum() -> tensor(13.0) (Scalar)\n",
        "y.backward() calculates the gradient of (x_1^2 + x_2^2) with respect to x_1 and x_2.\n",
        "Again, d(x_1^2 + x_2^2)/dx_1 = 2*x_1 = 2*3 = 6\n",
        "And d(x_1^2 + x_2^2)/dx_2 = 2*x_2 = 2*2 = 4\n",
        "xt.grad will still be tensor([6.0, 4.0]).\n",
        "In both cases, the scalar function whose gradient is being computed\n",
        "is ultimately f(x_1, x_2) = x_1^2 + x_2^2. The sum() operation just\n",
        "moves from being an explicit step outside the function definition\n",
        "to being an implicit step within the function definition.\n",
        "The mathematical result of the gradient calculation remains\n",
        "consistent because the underlying scalar function (the sum of squares) is the same.'''\n",
        "\n",
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "from fastai import *\n",
        "import fastbook\n",
        "from fastbook import *\n",
        "\n",
        "def function(x):\n",
        "  return (x**2).sum()\n",
        "\n",
        "xt = tensor([3.0,2.0]).requires_grad_(True) # a variable has been flagged for calcualting the gradient\n",
        "print(function(xt)) # normal fnction calcualtion\n",
        "\n",
        "y = function(xt)\n",
        "y.backward()\n",
        "print(xt.grad)\n",
        "\n"
      ],
      "metadata": {
        "id": "U-_lkfEffylv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradients only tell us the slope of our function, they don't actually tell us exactly how far to adjust the parameters. But it gives us some idea of how far; if the slope is very large, then that may suggest that we have more adjustments to do, whereas if the slope is very small, that may suggest that we are close to the optimal value."
      ],
      "metadata": {
        "id": "Z21c_xDp6R72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now implementing the simulation of stocastic gradient decent\n",
        "\n",
        "time = torch.arange(0,20).float();\n",
        "time\n"
      ],
      "metadata": {
        "id": "MiopBviB3SK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1\n",
        "plt.scatter(time, speed)"
      ],
      "metadata": {
        "id": "AlARZFww0zEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(t, params):\n",
        "    a,b,c = params\n",
        "    return a*(t**2) + (b*t) + c"
      ],
      "metadata": {
        "id": "iNhR8nR514Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(preds,targets):\n",
        "  return ((preds - targets)**2).mean()"
      ],
      "metadata": {
        "id": "AlCMPpuU2jAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize\n",
        "parms = torch.randn(3).requires_grad_()\n",
        "orig_parms = parms.clone()\n",
        "\n",
        "# predict\n",
        "predictions = f(time, parms)\n",
        "\n",
        "\n",
        "predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "yceUcA094Rkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def show_preds(preds, ax=None):\n",
        "#     # if ax is None: ax=plt.subplots()[1]\n",
        "#     plt.scatter(time, speed)\n",
        "#     plt.scatter(time, to_np(preds), color='red')\n",
        "#     # plt.set_ylim(-300,100)\n",
        "\n",
        "\n",
        "def show_preds(preds, ax=None):\n",
        "    if ax is None: ax=plt.subplots()[1]\n",
        "    ax.scatter(time, speed)\n",
        "    ax.scatter(time, to_np(preds), color='red')\n",
        "    ax.set_ylim(-300,100)"
      ],
      "metadata": {
        "id": "xA38m0UF5Q5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_preds(predictions)"
      ],
      "metadata": {
        "id": "PL4H6Reu58Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = mse(predictions, speed)\n",
        "loss"
      ],
      "metadata": {
        "id": "nIjUuqMo6DVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "9y_VZAaJ6fUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parms.grad"
      ],
      "metadata": {
        "id": "0uFBRSHS6p2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now multiplying by the learning rate\n",
        "parms.grad * 1e-5"
      ],
      "metadata": {
        "id": "lpHKImtL6syp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parms"
      ],
      "metadata": {
        "id": "vMt_AXnT61eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parms.grad.data"
      ],
      "metadata": {
        "id": "HksEbqPH65h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parms.data"
      ],
      "metadata": {
        "id": "ia37j1el7xOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parms.data -= 1e-5 * parms.grad.data"
      ],
      "metadata": {
        "id": "U7MSGfdR8HYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parms.data"
      ],
      "metadata": {
        "id": "3cpbgrl28RqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parms.grad = None"
      ],
      "metadata": {
        "id": "3wni9Mna8Sks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parms.grad"
      ],
      "metadata": {
        "id": "lk6ZKSfO8Wd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now check the mse\n",
        "predictions = f(time, parms)\n",
        "loss = mse(predictions, speed)\n",
        "loss"
      ],
      "metadata": {
        "id": "-ZZOyId28YV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_preds(predictions)"
      ],
      "metadata": {
        "id": "mBH6iZv28eIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now encapsulating the above in to a function\n",
        "def weights_update(parms, show = True):\n",
        "  predictions = f(time, parms)\n",
        "  loss = mse(predictions, speed)\n",
        "  if show:\n",
        "    print(loss)\n",
        "\n",
        "  loss.backward()\n",
        "  parms.data -= 1e-5*parms.grad.data\n",
        "  parms.grad = None\n",
        "  return predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "YsqQ4dLM8itA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100): weights_update(parms, False)"
      ],
      "metadata": {
        "id": "cCnVZ3dW-KTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_,axs = plt.subplots(1,4,figsize=(12,3))\n",
        "for ax in axs:\n",
        "  show_preds(weights_update(parms, False), ax)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "bvM0ZGtN-PkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now implementing the loss and stochastic gradient decent with MNIST Dataset"
      ],
      "metadata": {
        "id": "T_yiFDoGPnYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "from fastai import *\n",
        "import fastbook\n",
        "from fastbook import *"
      ],
      "metadata": {
        "id": "HDQQDMy4UHQB"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.MNIST_SAMPLE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "0tvkUnF6ULfd",
        "outputId": "a4be11c7-39da-4725-b6a7-c937c460b6dd"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<div><progress max=\"3214948\" value=\"3219456\"></progress> 100.14% [3219456/3214948 00:00&lt;00:00]</div>"
            ],
            "text/html": [
              "<div><progress max=\"3214948\" value=\"3219456\"></progress> 100.14% [3219456/3214948 00:00&lt;00:00]</div>"
            ],
            "text/markdown": "```html\n<div>\n<progress max=\"3214948\" value=\"3219456\"></progress> 100.14% [3219456/3214948 00:00&lt;00:00]</div>\n\n```"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path.ls()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVEfu4crUaCT",
        "outputId": "ee10b19a-ed34-46aa-e0ce-42b9bb1cd8a1"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Path('/root/.fastai/data/mnist_sample/labels.csv'), Path('/root/.fastai/data/mnist_sample/valid'), Path('/root/.fastai/data/mnist_sample/train')]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_threes = torch.stack([tensor(Image.open(o)) for o in (path/'train'/'3').ls().sorted()])\n",
        "\n",
        "label_3 = tensor([1]*len(stacked_threes)).reshape(-1,1) #unsqeeze(1)\n",
        "\n",
        "stacked_sevens = torch.stack([tensor(Image.open(o)) for o in (path/'train'/'7').ls().sorted()])\n",
        "\n",
        "label_7 = tensor([0]*len(stacked_sevens)).reshape(-1,1)"
      ],
      "metadata": {
        "id": "6vW3-KE2R1dS"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Take the first set of pixels\n",
        "img = stacked_threes[0]\n",
        "print(img.shape)\n",
        "\n",
        "# 2. Fold it into a 28x28 square (it's currently a flat line!)\n",
        "# img_square = img.view(28,28)\n",
        "\n",
        "# 3. Paint it!\n",
        "plt.imshow(img, cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "fvGhXs9NVUZI",
        "outputId": "d5e87999-a57a-41e5-b9ac-22fc853ca6ef"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([28, 28])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7baabfc0acc0>"
            ]
          },
          "metadata": {},
          "execution_count": 112
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n",
        "train_y = torch.cat([label_3, label_7])"
      ],
      "metadata": {
        "id": "QEXa4U07APob"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlaCmR1CQrRM",
        "outputId": "1c4da33d-5593-4940-9990-60d170864bb6"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12396, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y.shape"
      ],
      "metadata": {
        "id": "4Baqnmk6bQ4D",
        "outputId": "427f2485-066f-4276-b7c5-e2c92562d15c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12396, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# converting back to 28x28\n",
        "\n",
        "train_x.view(-1,28,28).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUGn6m7TVA_E",
        "outputId": "5b889015-9fd5-48a9-b1b6-f801af664b1b"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12396, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dset = list(zip(train_x,train_y)) # will be used later for data loader\n"
      ],
      "metadata": {
        "id": "WnEdhcsyYE6o",
        "outputId": "88ee431d-332a-4466-a9d5-7924b6e078cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2706442802.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jb8KCmqtbtqh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}